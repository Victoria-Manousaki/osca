<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="VLMAH: Visual-Linguistic Modeling of Action History
for Effective Action Anticipation">
  <meta name="keywords" content="Action anticipation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Anticipating Object State Changes</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Anticipating Object State Changes</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="http://users.ics.forth.gr/~vmanous/">Victoria Manousaki*</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://bouclas.github.io/">Konstantinos Bacharidis*</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="http://users.ics.forth.gr/~papoutsa/">Konstantinos Papoutsakis</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://users.ics.forth.gr/~argyros/index.html">Antonis Argyros</a><sup>1,2</sup>
            </span>
            
          </div>

          <div class="is-size-5 publication-authors">
		  <span class="author-block"><sup></sup>vmanous@ics.forth.gr, kbach@ics.forth.gr, papoutsa@ics.forth.gr, argyros@ics.forth.gr</span>
		  <span class="author-block"><sup></sup> </span> <br> <br>
            <span class="author-block"><sup>1</sup>Computer Science Department, University of Crete, Greece</span>
            <span class="author-block"><sup>2</sup>Institute of Computer Science, FORTH, Greece</span>
			<span class="author-block"><sup>3</sup>Department of Management, Science & Technology, Hellenic Mediterranean University, Greece</span>
			
			<span class="author-block"><sup>*</sup>Equal Contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="http://users.ics.forth.gr/~argyros/mypapers/2023_10_ACVR_Manousaki.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
         
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Victoria-Manousaki/VLMAH"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code </span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Anticipating object state changes in images and videos is a challenging problem whose solution has important 
			implications in vision-based scene understanding, automated monitoring systems, and action planning. In this work, 
			we propose the first method for solving this problem. The proposed method predicts object state changes that will 
			occur in the near future as a result of yet unseen human actions. To address this new problem, we propose a novel 
			framework that integrates learnt visual features that represent the recent visual information, with natural language 
			(NLP) features that represent past object state changes and actions. Leveraging the extensive and challenging Ego4D 
			dataset which provides a large-scale collection of first-person perspective videos across numerous interaction scenarios, 
			we introduce new curated annotation data for the object state change anticipation task. An extensive experimental evaluation 
			was conducted that demonstrates the efficacy of the proposed method in predicting object state changes in dynamic scenarios. 
			The proposed work underscores the potential of integrating video and linguistic cues to enhance the predictive performance 
			of video understanding systems. Moreover, it lays the groundwork for future research on the new task of object state change 
			anticipation. The source code and the new annotation data (Ego4D-OSCA) will be made publicly available.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

   
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
<div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Problem Definition</h2>
        <div class="content has-text-justified">
<br/>
        <!-- Interpolating. -->
    
        
  
            <img src="./static/images/problem.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
    
		<div class="content has-text-justified">
          <p>
           We introduce a new problem, that of anticipating object state changes in videos of
			procedural activities. At a decision point which is the start time of the next, yet unobserved
			action, the goal is to predict whether there will be a state change related to the next active
			object and, if yes, estimate its type. An object state change is defined as the transition from
			a pre-state (initial) to a post-state (final) that occurs at the Point of No Return (PNR) time
			during a state-modifying action.
          </p>
      </div>
      </div>
    </div>
    <!--/ Animation. -->


  </div>
</section>







<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
<br/>
        <!-- Interpolating. -->
    
        
  
            <img src="./static/images/ACVR.PNG"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
    
		<div class="content has-text-justified">
          <p>
           The proposed VLMAH architecture. The Visual Action module and the Linguistic Action History module are presented. For the
Meccano dataset, the encoders of the action module, generate Object, Hands, Gaze representations, whereas for the Assembly-101 dataset,
there is a single encoder network, TSM [27] while representations are split into 3 sub-sequences. The detail
level regarding the textual label descriptions is adaptable to the anticipation task at hand (action, motion motif (verb), or object (noun)).
The final format also includes two special labels (START, END) that indicate the start and end of the action history sequence.
          </p>
        </div>
        <br/>
		</div>
    </div>
    <!--/ Animation. -->
   


  </div>
</section>
















<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{under-review,
  author    = {},
  title     = {Anticipating Object State Changes},
  journal   = {Under-Review},
  year      = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            The source code of this webpage is from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
